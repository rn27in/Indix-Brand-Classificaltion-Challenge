# -*- coding: utf-8 -*-
"""
Created on Fri Oct 21 12:43:45 2016

@author: rohit.r
"""

#####Procedure for building models for first 1000 labels######################################
from collections import Counter
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
import numpy as np
from sklearn.externals import joblib
from sklearn.metrics import accuracy_score
from sklearn.externals import joblib

header_row = ['product_title', 'brand_id', 'category_id']
data = pd.read_csv('D:/Projects/External/Indix/Indix 2/Data/classification_train.tsv', sep = '\t', header = None, names = header_row)
data = data[data.brand_id != 'bid']
y = data['brand_id'].values

###this function has been taken from Andreas Mueller
def deduplicate(comments, category_id,labels):
    hashes = np.array([hash(c) for c in comments])
    unique_hashes, indices = np.unique(hashes, return_inverse=True)
    doubles = np.where(np.bincount(indices) > 1)[0]
    mask = np.ones(len(comments), dtype=np.bool)
    # for each double entry
    for i in doubles:
        # mask out all but the first occurence
        not_the_first = np.where(indices == i)[0][1:]
        mask[not_the_first] = False
    return comments[mask],category_id[mask], labels[mask]

prod_title,cat_id,labels = deduplicate(data.product_title,data.category_id, data['brand_id'].values)

#################Assorting labels appropriately####################################### 
y = []
for i in labels:
    try:
        y.append(int(i))
    
    except ValueError :
        y.append(999999999)

c = Counter(y)

x_val,y_val = [list(i) for i in zip(*c.most_common(1000))]

#####################Creating deduplicated dataframe #########################################
prod_title = prod_title.reset_index(drop = True)
prod_title = prod_title.product_title

cat_id = cat_id.reset_index(drop = True)

dedups_df = pd.concat([prod_title,cat_id,y1] ,axis = 1)

dedups_df = dedups_df.rename(columns={'product_title': 'product_title', 'category_id': 'category_id', 0:'labels'})

###Filtering dataframe only on top 1000 categories########################################################
dedups_df_final = dedups_df[(dedups_df['labels'].isin(x_val))]

temp = pd.get_dummies(dedups_df_final['labels'])

dedups_df_final = pd.concat([dedups_df_final,temp], axis = 1)

y_labels = pd.get_dummies(dedups_df_final['labels'])
####################################Building one vs all classifier#############################################

test = dedups_df_final.iloc[616364:,0]
#prediction_on_test = np.array([])
#accuracy_sc = []
dir = 'C:/Indix Dumps/Dumps/Test_500/'
chunks = 8
vals_chunk = y_labels.shape[0]/chunks 

x_val = joblib.load('C:/Indix Dumps/x_val')

for t in range(chunks-1):
    initial = t*vals_chunk
    final = (t+1)*vals_chunk
    temp_data = dedups_df_final.iloc[initial:final,0]
    vect = TfidfVectorizer(max_features = 20000, ngram_range = (1,2), sublinear_tf=True)
    temp = vect.fit_transform(temp_data)
    temp_test = vect.transform(test)
    joblib.dump(vect,dir +'vect_'+str(t))
    del vect
    for j in x_val[0:1000]:
        y_labels_temp = y_labels.iloc[initial:final,np.where(y_labels.columns == j)[0]]
        y_labels_temp = y_labels_temp.values.reshape(1,y_labels_temp.shape[0])[0]
        clf = SGDClassifier(loss = "modified_huber")
        clf.fit(temp,y_labels_temp )
        joblib.dump(clf, dir +'sgd_' +str(j)+ '_'+ str(t))
        del clf

###############Building the model on the last chunk which was earlier used for testing#################
temp_data = dedups_df_final.iloc[616364:,0]
vect = TfidfVectorizer(max_features = 30000, ngram_range = (1,2), sublinear_tf= True)
temp = vect.fit_transform(temp_data)
joblib.dump(vect,dir +'vect_'+str(t))
for j in x_val[0:1000]:
    y_labels_temp = y_labels.iloc[616364:,np.where(y_labels.columns == j)[0]]
    y_labels_temp = y_labels_temp.values.reshape(1,y_labels_temp.shape[0])[0]
    clf = SGDClassifier(loss = "modified_huber")
    clf.fit(temp,y_labels_temp )
    joblib.dump(clf, dir +'sgd_' +str(j)+ '_'+ str(t))
    del clf

#################################Building classifiers from 1000 to 5000#########################################
c = Counter(dedups_df['labels'])

x_val,y_val = [list(i) for i in zip(*c.most_common(5000))]

x_val_final = x_val[1000:]

dedups_df_final = dedups_df[(dedups_df['labels'].isin(x_val_final))]

y_labels = pd.get_dummies(dedups_df_final['labels'])




chunks = 3
vals_chunk = y_labels.shape[0]/chunks 

for t in range(chunks):
    if t <2:
        initial = t*vals_chunk
        final = (t+1)*vals_chunk
    else:
        initial = t*vals_chunk
        final = y_labels.shape[0]
    
    print t,initial,final    
    
###Building classifiers on entire chunk of 1,17,000 records for 1000 to 5000 labels##############
t = 0
vect = TfidfVectorizer(max_features = 20000, ngram_range = (1,2),sublinear_tf=True)
temp = vect.fit_transform(dedups_df_final['product_title'])
joblib.dump(vect, 'D:/hito/Models_1000/vect_0_final_4000')
for j in x_val_final:
    clf = SGDClassifier(loss = "modified_huber")
    y_labels_temp = y_labels.iloc[:,np.where(y_labels.columns == j)[0]]
    y_labels_temp = y_labels_temp.values.reshape(1,y_labels_temp.shape[0])[0]
    clf.fit(temp,y_labels_temp )
    joblib.dump(clf, 'D:/hito/Models_1000/' +'sgd_' +str(j)+ '_'+ str(t))

#######################################################################



